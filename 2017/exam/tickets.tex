\documentclass[15pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{graphicx}
\usepackage[russian]{babel}
\usepackage{amssymb,amsmath,mathrsfs}
\usepackage{longtable}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\definecolor{todo}{RGB}{220,39,57}
\definecolor{important}{RGB}{57,39,220}
\newcommand{\alert}[1]{\textcolor{todo}{TODO: #1}}
\newcommand{\imp}[1]{\textcolor{important}{#1}}
\textheight=24cm
\textwidth=16cm
\oddsidemargin=0pt
\topmargin=-1.5cm
\parindent=0pt
\parskip=0pt
\tolerance=2000
\flushbottom
\newtheorem{Th}{Теорема}[section]
\newtheorem{Def}{Определение}[section]
\newtheorem{Lemm}{Лемма}[section]
\newtheorem{Col}{Следствие}[section]
\newtheorem{St}{Утверждение}[section]
\newcommand{\Expect}{\mathsf{E}}

\title{ФИВТ 2017 \\ Машинное обучение\\ Экзаменационные вопросы.}
\date{}

\begin{document}

\maketitle
\begin{enumerate}
  \item Supervised и unsupervised learning. Стандартные задачи (классификация, регрессия, кластеризация).
      Простые модели (kNN, naive bayes, linear regression, k-Means).
      Оценка качества - кросс-валидация, кривые обучения.
      Переобучение и недообучение, как их детектировать.

  \item Метрики качества в задачах классификации и регрессии: accuracy, precision, recall, F1, ROC-AUC, log loss, MSE, MAE, квантильная, MAPE, SMAPE.
      Когда какая из метрик предпочтительней.
      К оценке каких параметров распределений ответов приводят различные метрики.

  \item Извлечение признаков (на примере текста, изображений, звука) и предобработка признаков (на примере работы с разреженными и категориальными признаками).

  \item Решающие деревья. Как работает уже построенное решающее дерево.
      Рекурсивное построение деревьев.
      Энтропийный критерий и критерий gini.
      Классификационные и регрессионные деревья: в чем различия.
      Настройка гиперпараметров решающего дерева.
      Преимущества и недостатки деревьев.

  \item Общие методы построения композиций: блэндинг, стэкинг, бэггинг и бустинг.
      Bias-variance trade-off (без вывода).
      Анализ бустинга и бэггинга с помощью bias-variance trade-off.

  \item Бэггинг и Random Forest. Связь корреляции между ответами моделей и качеством модели в бэггинге.
      Сравнение Random Forest и GBDT (подбор параметров, переобучение).

  \item Бустинг и GBM. Выбор параметров в ансамблях решающих деревьев.
  
  \item Линейные модели в задачах классификации и регрессии: функции потерь, регуляризаторы, оптимизационные задачи.
      Стохастический градиентный спуск.

  \item Линейная регрессия. Геометрический и аналитический вывод формулы для весов признаков.
      Регуляризация в линейной регрессии: гребневая регрессия и LASSO.

  \item Логистическая регрессия. Варианты записи оптимизационной задачи.
      Оценка вероятности принадлежности к классу.
      Настройка параметров с помощью стохастического градиентного спуска.

  \item Метод опорных векторов: оптимизационная задача в условной и безусловной форме.
      Опорные векторы (достаточно записать и продифференцировать Лагранжиан; оптимизационная задача, выраженная через двойственные переменные - опционально).
      Идея Kemel Trick.

  \item Нейронные сети, обучение (backprop), слои для нейронных сетей (dense, conv, pooling, batchnorm), нелинейности (relu vs sigmoid, softmax), функции потерь (logloss, l2, hinge).

  \item Нейронные сети, обучение (backprop), оптимизация для нейронных сетей (sg, msg, nmsg, rmsprop, adam), регуляризация нейросетей (dropout, dropconnect, l1, l2, batchnorm).

  \item Нейронные сети, обучение (backprop), современные сверточные слои нейронных сетей (vgg, resnet, inception) и детали обучения (batchnorm, pretrainig).

  \item Нейронные сети, обучение (backprop), сверточные слои, перенос стилей (генеративные модели), визуализация, автоэнкодеры.

  \item Рекуррентные НС, обучение (backprop tt), отличие от сверточных, разновидности рекуррентных слоев (RNN, LSTM, GRU), аннотация изображений, перевод, диалоговые системы.

  \item Задача снижения размерности пространства признаков.
      Идея метода главных компонент (PCA).
      Связь PCA и сингулярного разложения матрицы признаков (SVD).

  \item Вычисление SVD в пространствах высокой размерности методом стохастического градиента (SG SVD).

  \item Идея методов SNE, tSNE, принципиальные отличия от PCA.

  \item Задача кластеризации. Аггломеративная и дивизионная кластеризация.

  \item Кластеризация с помощью EM-алгоритма (без вывода М-шага).
      Формула Ланса-Уилльямса.
      
  \item Алгоритмы k-Means и DBSCAN.
  
  
\end{enumerate}

\end{document}
