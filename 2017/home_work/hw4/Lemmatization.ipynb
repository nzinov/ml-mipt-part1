{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                  X                 y    y1    y2    y3    y4\n",
      "0        1.0       vergognerete      vergognare+V   NaN   NaN   NaN   NaN\n",
      "1        2.0       amnistiavate      amnistiare+V   NaN   NaN   NaN   NaN\n",
      "2        3.0        menomazione     menomazione+N   NaN   NaN   NaN   NaN\n",
      "3        4.0         sfaldavamo        sfaldare+V   NaN   NaN   NaN   NaN\n",
      "4        5.0         sfodererei       sfoderare+V   NaN   NaN   NaN   NaN\n",
      "5        6.0         ascondesti       ascondere+V   NaN   NaN   NaN   NaN\n",
      "6        7.0      edifichereste       edificare+V   NaN   NaN   NaN   NaN\n",
      "7        8.0         maschieran       maschiare+V   NaN   NaN   NaN   NaN\n",
      "8        9.0      transennasser     transennare+V   NaN   NaN   NaN   NaN\n",
      "9       10.0         computando       computare+V   NaN   NaN   NaN   NaN\n",
      "10      11.0         accudisser        accudire+V   NaN   NaN   NaN   NaN\n",
      "11      12.0       diromperanno       dirompere+V   NaN   NaN   NaN   NaN\n",
      "12      13.0      intercollegai  intercollegare+V   NaN   NaN   NaN   NaN\n",
      "13      14.0         integrando       integrare+V   NaN   NaN   NaN   NaN\n",
      "14      15.0      sbramerebbero        sbramare+V   NaN   NaN   NaN   NaN\n",
      "15      16.0  stravaccherebbero     stravaccare+V   NaN   NaN   NaN   NaN\n",
      "16      17.0     oltrepassasser    oltrepassare+V   NaN   NaN   NaN   NaN\n",
      "17      18.0        cauzionavan      cauzionare+V   NaN   NaN   NaN   NaN\n",
      "18      19.0        scarrozzata     scarrozzare+V   NaN   NaN   NaN   NaN\n",
      "19      20.0     intossicassero     intossicare+V   NaN   NaN   NaN   NaN\n",
      "20      21.0           incurvan       incurvare+V   NaN   NaN   NaN   NaN\n",
      "21      22.0            zufolan        zufolare+V   NaN   NaN   NaN   NaN\n",
      "22      23.0        impiastrate     impiastrare+V   NaN   NaN   NaN   NaN\n",
      "23      24.0    trivellerebbero      trivellare+V   NaN   NaN   NaN   NaN\n",
      "24      25.0      pizzicottassi    pizzicottare+V   NaN   NaN   NaN   NaN\n",
      "25      26.0    spoliticizzasti  spoliticizzare+V   NaN   NaN   NaN   NaN\n",
      "26      27.0             rombar         rombare+V   NaN   NaN   NaN   NaN\n",
      "27      28.0     scapricceremmo    scapricciare+V   NaN   NaN   NaN   NaN\n",
      "28      29.0    intrometterebbe    intromettere+V   NaN   NaN   NaN   NaN\n",
      "29      30.0     lavoricchiassi   lavoricchiare+V   NaN   NaN   NaN   NaN\n",
      "...      ...                ...               ...   ...   ...   ...   ...\n",
      "120867   NaN            caletta       calettare+V  None  None  None  None\n",
      "120868   NaN          esagerate       esagerato+A  None  None  None  None\n",
      "120869   NaN          idealista       idealista+N  None  None  None  None\n",
      "120870   NaN            prodiga         prodigo+A  None  None  None  None\n",
      "120871   NaN         dissidenti      dissidente+N  None  None  None  None\n",
      "120872   NaN           favoriti        favorito+A  None  None  None  None\n",
      "120873   NaN              rossa           rossa+N  None  None  None  None\n",
      "120874   NaN              finto           finto+A  None  None  None  None\n",
      "120875   NaN              verdi           verde+A  None  None  None  None\n",
      "120876   NaN            setacci        setaccio+N  None  None  None  None\n",
      "120877   NaN     corrispondenti  corrispondente+N  None  None  None  None\n",
      "120878   NaN       appassionate    appassionare+V  None  None  None  None\n",
      "120879   NaN              tonda           tonda+N  None  None  None  None\n",
      "120880   NaN             spilla        spillare+V  None  None  None  None\n",
      "120881   NaN               doli            dolo+N  None  None  None  None\n",
      "120882   NaN          siederete    soprassedere+V  None  None  None  None\n",
      "120883   NaN            zingaro         zingaro+N  None  None  None  None\n",
      "120884   NaN          congiunti     congiungere+V  None  None  None  None\n",
      "120885   NaN            vestita         vestire+V  None  None  None  None\n",
      "120886   NaN            vestita         vestito+A  None  None  None  None\n",
      "120887   NaN           testarda        testarda+N  None  None  None  None\n",
      "120888   NaN            orrendi         orrendo+N  None  None  None  None\n",
      "120889   NaN            zingare         zingara+N  None  None  None  None\n",
      "120890   NaN              mozzo         mozzare+V  None  None  None  None\n",
      "120891   NaN            logiche          logica+N  None  None  None  None\n",
      "120892   NaN          sfegatate       sfegatare+V  None  None  None  None\n",
      "120893   NaN             encomi       encomiare+V  None  None  None  None\n",
      "120894   NaN           sfiducia      sfiduciare+V  None  None  None  None\n",
      "120895   NaN          stravolto       stravolto+A  None  None  None  None\n",
      "120896   NaN              colli           collo+N  None  None  None  None\n",
      "\n",
      "[120897 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pandas.read_csv(\"task2_lemmas_train\",names=[\"ID\", \"X\", \"y\", \"y1\", \"y2\", \"y3\", \"y4\"], skiprows=1)\n",
    "train = train\n",
    "test = pandas.read_csv(\"task2_lemmas_test\")\n",
    "ans = pandas.read_csv(\"task2_lemmas_sample_submission\")\n",
    "additional = []\n",
    "for index, row in train.iterrows():\n",
    "    if isinstance(row.y1, str):\n",
    "        additional.append((row.X, row.y1))\n",
    "    if isinstance(row.y2, str):\n",
    "        additional.append((row.X, row.y2))\n",
    "    if isinstance(row.y3, str):\n",
    "        additional.append((row.X, row.y3))\n",
    "    if isinstance(row.y4, str):\n",
    "        additional.append((row.X, row.y4))\n",
    "for el in additional:\n",
    "    train.loc[len(train)] = {\"Id\": None, \"X\": el[0], \"y\": el[1], \"y1\": None, \"y2\": None, \"y3\": None, \"y4\": None}\n",
    "print train\n",
    "train['tag'] = train.y.apply(lambda s: s[-1])\n",
    "train.y = train.y.apply(lambda s: s[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtractFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.extract(word) for word in X]\n",
    "\n",
    "    def extract(self, word):\n",
    "        features = defaultdict(lambda: 0)\n",
    "        if word[0].isupper() and not word.isupper():\n",
    "            features['capitalized'] = 1\n",
    "        if word.isupper():\n",
    "            features['allcaps'] = 1\n",
    "        word = word.lower()\n",
    "        for i in range(min(len(word), 7)):\n",
    "            features[\"pref\" + word[:i]] += 1\n",
    "            features[\"suf\" + word[-i:]] += 1\n",
    "            if i > 0:\n",
    "                features[\"word\" + word[:-i] + \"*\"*i] += 1\n",
    "        for length in range(1, 6):\n",
    "            for i in range(len(word) - length + 1):\n",
    "                features[word[i:i + length]] += 1\n",
    "            for offset in range(1, 4):\n",
    "                if len(word) - offset - length < 0:\n",
    "                    continue\n",
    "                features[word[len(word) - offset - length: len(word) - offset] + \"*\"*offset] += 1\n",
    "        features[\"length\"] = len(word)\n",
    "        features[\"rect_len\"] = max(len(word) - 10, 0)\n",
    "        return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.X = [unicode(x, \"utf-8\") for x in train.X]\n",
    "test.X = [unicode(x, \"utf-8\") for x in test.X]\n",
    "train.y = [unicode(x, \"utf-8\") for x in train.y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(ExtractFeatures().transform(train.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = make_pipeline(sklearn.preprocessing.MaxAbsScaler(), LogisticRegression(class_weight='balanced'))\n",
    "est.fit(train_data, train.tag)\n",
    "#sklearn.model_selection.cross_val_score(est, train_data, train.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longest_common_substring(s1, s2):\n",
    "    m = [[0] * (1 + len(s2)) for i in xrange(1 + len(s1))]\n",
    "    longest, x_longest, y_longest = 0, 0, 0\n",
    "    for x in xrange(1, 1 + len(s1)):\n",
    "        for y in xrange(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "                    y_longest = y\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    return x_longest, y_longest, longest\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __hash__(self):\n",
    "        return unicode(self).__hash__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return unicode(self) == unicode(other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__unicode__()\n",
    "\n",
    "class SplitNode(Node):\n",
    "    def __init__(self, bounds, left, right):\n",
    "        self.bounds = bounds\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def convert(self, word):\n",
    "        if self.bounds[1] > len(word):\n",
    "            return None\n",
    "        left = self.left.convert(word[:self.bounds[0]])\n",
    "        if left is None:\n",
    "            return None\n",
    "        right = self.right.convert(word[self.bounds[1]:])\n",
    "        if right is None:\n",
    "            return None\n",
    "        return \"\".join([left, word[self.bounds[0]:self.bounds[1]], right])\n",
    "    \n",
    "    def alignment(self, word):\n",
    "        al = self.left.alignment(word[:self.bounds[0]]) + self.right.alignment(word[self.bounds[1]:])\n",
    "        if self.bounds[1] - self.bounds[0] > 3:\n",
    "            al.append(u\"R{0}\".format(word[self.bounds[0]:self.bounds[1]]))\n",
    "        return al\n",
    "\n",
    "    def __unicode__(self):\n",
    "        return u\"{}<{}><{}>\".format(self.bounds, self.left, self.right)\n",
    "\n",
    "class ReplaceNode(Node):\n",
    "    def __init__(self, pattern, target):\n",
    "        self.pattern = pattern\n",
    "        self.target = target\n",
    "\n",
    "    def convert(self, word):\n",
    "        if word == self.pattern:\n",
    "            return self.target\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def alignment(self, word):\n",
    "        if len(self.pattern) == 0 and len(self.target) == 0:\n",
    "            return []\n",
    "        return [unicode(self)]\n",
    "\n",
    "    def __unicode__(self):\n",
    "        return u\"{}/{}\".format(self.pattern, self.target)\n",
    "\n",
    "\n",
    "def make_tree(word, lemma):\n",
    "    x_longest, y_longest, longest = longest_common_substring(word, lemma)\n",
    "    if longest <= 2:\n",
    "        return ReplaceNode(word, lemma)\n",
    "    else:\n",
    "        left = make_tree(word[:x_longest - longest], lemma[:y_longest - longest])\n",
    "        right = make_tree(word[x_longest:], lemma[y_longest:])\n",
    "        bounds = (x_longest - longest, x_longest)\n",
    "        return SplitNode(bounds, left, right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hasher = FeatureHasher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_count = defaultdict(lambda: 0)\n",
    "for word, lemma in zip(train.X, train.y):\n",
    "    tree_count[make_tree(word, lemma)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3637\n"
     ]
    }
   ],
   "source": [
    "trees = [k for k, v in tree_count.iteritems() if v > 0]\n",
    "print len(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(features, feature, tag=None):\n",
    "    features[feature] = 1\n",
    "    if tag is not None:\n",
    "        features[tag+feature] = 1\n",
    "\n",
    "def get_features(word, lemma, tag, tree=None):\n",
    "    features = defaultdict(lambda: 0)\n",
    "    if tree is None:\n",
    "        tree = make_tree(word, lemma)\n",
    "    add_feature(features, unicode(tree), tag)\n",
    "    for el in tree.alignment(word):\n",
    "        add_feature(features, el, tag)\n",
    "        add_feature(features, u\"{}:{}\".format(el, tree), tag)\n",
    "        if el[0] == 'R':\n",
    "            add_feature(features, u\"{}->{}\".format(el, lemma), tag)\n",
    "    add_feature(features, lemma, tag)\n",
    "    for i in range(1, min(len(lemma), 5)):\n",
    "        #add_feature(features, \"pref\" + lemma[:i], tag)\n",
    "        add_feature(features, \"suf\" + lemma[-i:], tag)\n",
    "    features[\"tag_score\"] = est.predict_proba(vectorizer.transform(ExtractFeatures().extract(word)))[0, list(est.classes_).index(tag)]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train[:80000]\n",
    "X_test = train[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 0\n",
      ". . . . . . . . . . 1000\n",
      ". . . . . . . . . . 2000\n",
      ". . . . . . . . . . 3000\n",
      ". . . . . . . . . . 4000\n",
      ". . . . . . . . . . 5000\n",
      ". . . . . . . . . . 6000\n",
      ". . . . . . . . . . 7000\n",
      ". . . . . . . . . . 8000\n",
      ". . . . . . . . . . 9000\n",
      ". . . . . . . . . . 10000\n",
      ". . . . . . . . . . 11000\n",
      ". . . . . . . . . . 12000\n",
      ". . . . . . . . . . 13000\n",
      ". . . . . . . . . . 14000\n",
      ". . . . . . . . . . 15000\n",
      ". . . . . . . . . . 16000\n",
      ". . . . . . . . . . 17000\n",
      ". . . . . . . . . . 18000\n",
      ". . . . . . . . . . 19000\n",
      ". . . . . . . . . . 20000\n",
      ". . . . . . . . . . 21000\n",
      ". . . . . . . . . . 22000\n",
      ". . . . . . . . . . 23000\n",
      ". . . . . . . . . . 24000\n",
      ". . . . . . . . . . 25000\n",
      ". . . . . . . . . . 26000\n",
      ". . . . . . . . . . 27000\n",
      ". . . . . . . . . . 28000\n",
      ". . . . . . . . . . 29000\n",
      ". . . . . . . . . . 30000\n",
      ". . . . . . . . . . 31000\n",
      ". . . . . . . . . . 32000\n",
      ". . . . . . . . . . 33000\n",
      ". . . . . . . . . . 34000\n",
      ". . . . . . . . . . 35000\n",
      ". . . . . . . . . . 36000\n",
      ". . . . . . . . . . 37000\n",
      ". . . . . . . . . . 38000\n",
      ". . . . . . . . . . 39000\n",
      ". . . . . . . . . . 40000\n",
      ". . . . . . . . . . 41000\n",
      ". . . . . . . . . . 42000\n",
      ". . . . . . . . . . 43000\n",
      ". . . . . . . . . . 44000\n",
      ". . . . . . . . . . 45000\n",
      ". . . . . . . . . . 46000\n",
      ". . . . . . . . . . 47000\n",
      ". . . . . . . . . . 48000\n",
      ". . . . . . . . . . 49000\n",
      ". . . . . . . . . . 50000\n",
      ". . . . . . . . . . 51000\n",
      ". . . . . . . . . . 52000\n",
      ". . . . . . . . . . 53000\n",
      ". . . . . . . . . . 54000\n",
      ". . . . . . . . . . 55000\n",
      ". . . . . . . . . . 56000\n",
      ". . . . . . . . . . 57000\n",
      ". . . . . . . . . . 58000\n",
      ". . . . . . . . . . 59000\n",
      ". . . . . . . . . . 60000\n",
      ". . . . . . . . . . 61000\n",
      ". . . . . . . . . . 62000\n",
      ". . . . . . . . . . 63000\n",
      ". . . . . . . . . . 64000\n",
      ". . . . . . . . . . 65000\n",
      ". . . . . . . . . . 66000\n",
      ". . . . . . . . . . 67000\n",
      ". . . . . . . . . . 68000\n",
      ". . . . . . . . . . 69000\n",
      ". . . . . . . . . . 70000\n",
      ". . . . . . . . . . 71000\n",
      ". . . . . . . . . . 72000\n",
      ". . . . . . . . . . 73000\n",
      ". . . . . . . . . . 74000\n",
      ". . . . . . . . . . 75000\n",
      ". . . . . . . . . . 76000\n",
      ". . . . . . . . . . 77000\n",
      ". . . . . . . . . . 78000\n",
      ". . . . . . . . . . 79000\n",
      ". . . . . . . . . . 80000\n",
      ". . . . . . . . . . 81000\n",
      ". . . . . . . . . . 82000\n",
      ". . . . . . . . . . 83000\n",
      ". . . . . . . . . . 84000\n",
      ". . . . . . . . . . 85000\n",
      ". . . . . . . . . . 86000\n",
      ". . . . . . . . . . 87000\n",
      ". . . . . . . . . . 88000\n",
      ". . . . . . . . . . 89000\n",
      ". . . . . . . . . . 90000\n",
      ". . . . . . . . . . 91000\n",
      ". . . . . . . . . . 92000\n",
      ". . . . . . . . . . 93000\n",
      ". . . . . . . . . . 94000\n",
      ". . . . . . . . . . 95000\n",
      ". . . . . . . . . . 96000\n",
      ". . . . . . . . . . 97000\n",
      ". . . . . . . . . . 98000\n",
      ". . . . . . . . . . 99000\n",
      ". . . . . . . . . . 100000\n",
      ". . . . . . . . . . 101000\n",
      ". . . . . . . . . . 102000\n",
      ". . . . . . . . . . 103000\n",
      ". . . . . . . . . . 104000\n",
      ". . . . . . . . . . 105000\n",
      ". . . . . . . . . . 106000\n",
      ". . . . . . . . . . 107000\n",
      ". . . . . . . . . . 108000\n",
      ". . . . . . . . . . 109000\n",
      ". . . . . . . . . . 110000\n",
      ". . . . . . . . . . 111000\n",
      ". . . . . . . . . . 112000\n",
      ". . . . . . . . . . 113000\n",
      ". . . . . . . . . . 114000\n",
      ". . . . . . . . . . 115000\n",
      ". . . . . . . . . . 116000\n",
      ". . . . . . . . . . 117000\n",
      ". . . . . . . . . . 118000\n",
      ". . . . . . . . . . 119000\n",
      ". . . . . . . . . . 120000\n",
      ". . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "def get_data():\n",
    "    for index, row in train.iterrows():\n",
    "        if index % 100 == 0:\n",
    "            print '.',\n",
    "        if index % 1000 == 0:\n",
    "            print index\n",
    "        word = row.X\n",
    "        has_true = False\n",
    "        skip = 1\n",
    "        for i in range(index % skip, len(trees), skip):\n",
    "            tree = trees[i]\n",
    "            lemma = tree.convert(word)\n",
    "            if lemma is None:\n",
    "                continue\n",
    "            for tag in est.classes_:\n",
    "                true = tag == row.tag and lemma == row.y\n",
    "                if true:\n",
    "                    has_true = True\n",
    "                y.append(int(true))\n",
    "                yield get_features(word, lemma, tag, tree)\n",
    "        if not has_true:\n",
    "            yield get_features(word, row.y, row.tag)\n",
    "            y.append(1)\n",
    "X = hasher.transform(get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 0\n",
      ". . . . . . . . . . 1000\n",
      ". . . . . . . . . . 2000\n",
      ". . . . . . . . . . 3000\n",
      ". . . . . . . . . . 4000\n",
      ". . . . . . . . . . 5000\n",
      ". . . . . . . . . . 6000\n",
      ". . . . . . . . . . 7000\n",
      ". . . . . . . . . . 8000\n",
      ". . . . . . . . . . 9000\n",
      ". . . . . . . . . . 10000\n",
      ". . . . . . . . . . 11000\n",
      ". . . . . . . . . . 12000\n",
      ". . . . . . . . . . 13000\n",
      ". . . . . . . . . . 14000\n",
      ". . . . . . . . . . 15000\n",
      ". . . . . . . . . . 16000\n",
      ". . . . . . . . . . 17000\n",
      ". . . . . . . . . . 18000\n",
      ". . . . . . . . . . 19000\n",
      ". . . . . . . . . . 20000\n",
      ". . . . . . . . . . 21000\n",
      ". . . . . . . . . . 22000\n",
      ". . . . . . . . . . 23000\n",
      ". . . . . . . . . . 24000\n",
      ". . . . . . . . . . 25000\n",
      ". . . . . . . . . . 26000\n",
      ". . . . . . . . . . 27000\n",
      ". . . . . . . . . . 28000\n",
      ". . . . . . . . . . 29000\n",
      ". . . . . .\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for index, row in test.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print '.',\n",
    "    if index % 1000 == 0:\n",
    "        print index\n",
    "    word = row.X\n",
    "    best_lemma = 'test'\n",
    "    best_p = 0\n",
    "    best_tag = 'N'\n",
    "    for tree in trees:\n",
    "        lemma = tree.convert(word)\n",
    "        if lemma is None:\n",
    "            continue\n",
    "        for tag in est.classes_:\n",
    "            p = model.predict_proba(hasher.transform([get_features(word, lemma, tag, tree)]))[0, 1]\n",
    "            if p > best_p:\n",
    "                best_p = p\n",
    "                best_tag = tag\n",
    "                best_lemma = lemma\n",
    "    answers.append(u\"{}+{}\".format(best_lemma, best_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans.Category = answers\n",
    "ans.to_csv(\"lemma.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
